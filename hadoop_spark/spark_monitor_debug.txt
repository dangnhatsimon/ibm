# Start a Spark Standalone Cluster
# Initialize the Cluster
# Stop any previously running containers with the command:
for i in `docker ps | awk '{print $1}' | grep -v CONTAINER`; do docker kill $i; done

# Remove any previously used containers:
docker rm spark-master spark-worker-1 spark-worker-2

# Start the Spark Master server:
docker run \
    --name spark-master \
    -h spark-master \
    -e ENABLE_INIT_DAEMON=false \
    -p 4040:4040 \
    -p 8080:8080 \
    -v `pwd`:/home/root \
    -d bde2020/spark-master:3.1.1-hadoop3.2

# Start a Spark Worker that will connect to the Master:
docker run \
    --name spark-worker-1 \
    --link spark-master:spark-master \
    -e ENABLE_INIT_DAEMON=false \
    -p 8081:8081 \
    -v `pwd`:/home/root \
    -d bde2020/spark-worker:3.1.1-hadoop3.2

# Connect a PySpark Shell to the Cluster and Open the UI
# Launch a PySpark shell in the running Spark Master container:
docker exec \
    -it `docker ps | grep spark-master | awk '{print $1}'` \
    /spark/bin/pyspark \
    --master spark://spark-master:7077

# Create a DataFrame in the shell with:
df = spark.read.csv("/home/root/cars.csv", header=True, inferSchema=True) \
    .repartition(32) \
    .cache()
df.show()

# Run an SQL Query and Debug in the Application UI
# Define a UDF to show engine type
from pyspark.sql.functions import udf
import time

@udf("string")
def engine(cylinders):
    time.sleep(0.2)  # Intentionally delay task
    eng = {4: "inline-four", 6: "V6", 8: "V8"}
    return eng.get(cylinders, "other")

df = df.withColumn("engine", engine("cylinders"))
dfg = df.groupby("cylinders")
dfa = dfg.agg({"mpg": "avg", "engine": "first"})
dfa.show()

# Add a Worker to the Cluster
# Open a new terminal by clicking on the menu item. Add a second worker to the cluster with the command in the new terminal:
docker run \
    --name spark-worker-2 \
    --link spark-master:spark-master \
    -e ENABLE_INIT_DAEMON=false \
    -p 8082:8082 \
    -d bde2020/spark-worker:3.1.1-hadoop3.2

# Click back to the first terminal that has the PySpark shell open to continue
dfa.show()